{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13422853,"sourceType":"datasetVersion","datasetId":8519412}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from datasets import load_dataset\n\n# Lá»‡nh ngáº¯n gá»n Ä‘á»ƒ táº£i toÃ n bá»™ dataset\n# Dá»¯ liá»‡u sáº½ Ä‘Æ°á»£c lÆ°u vÃ o bá»™ nhá»› (memory/cache) cá»§a session Kaggle\nalpaca_dataset = load_dataset(\"tatsu-lab/alpaca\")\n\n# In cáº¥u trÃºc Ä‘á»ƒ xÃ¡c nháº­n\nprint(alpaca_dataset) \n\n# Láº¥y 5 vÃ­ dá»¥ Ä‘áº§u tiÃªn tá»« split 'train' Ä‘á»ƒ xem dá»¯ liá»‡u\ntrain_split = alpaca_dataset['train']\nfirst_5_examples_dict = train_split[:5]\nprint(first_5_examples_dict)\n# má»™t táº­p há»£p cÃ¡c cáº·p chá»‰ dáº«n/cÃ¢u há»i vÃ  pháº£n há»“i/cÃ¢u tráº£ lá»i (Instruction-Output Pairs).","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-18T14:50:40.350812Z","iopub.execute_input":"2025-10-18T14:50:40.351033Z","iopub.status.idle":"2025-10-18T14:50:48.830451Z","shell.execute_reply.started":"2025-10-18T14:50:40.351011Z","shell.execute_reply":"2025-10-18T14:50:48.829670Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0c08b64bb394fbd87047e7d6ddf2d48"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00000-of-00001-a09b74b3ef9c3b(â€¦):   0%|          | 0.00/24.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d0e72bb592244baaad488e288eeeae0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/52002 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bbfc47344c745cfaa68b9160d874bd5"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['instruction', 'input', 'output', 'text'],\n        num_rows: 52002\n    })\n})\n{'instruction': ['Give three tips for staying healthy.', 'What are the three primary colors?', 'Describe the structure of an atom.', 'How can we reduce air pollution?', 'Describe a time when you had to make a difficult decision.'], 'input': ['', '', '', '', ''], 'output': ['1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.', 'The three primary colors are red, blue, and yellow.', 'An atom is made up of a nucleus, which contains protons and neutrons, surrounded by electrons that travel in orbits around the nucleus. The protons and neutrons have a positive charge, while the electrons have a negative charge, resulting in an overall neutral atom. The number of each particle determines the atomic number and the type of atom.', 'There are a number of ways to reduce air pollution, such as shifting to renewable energy sources, encouraging the use of public transportation, prohibiting the burning of fossil fuels, implementing policies to reduce emissions from industrial sources, and implementing vehicle emissions standards. Additionally, individuals can do their part to reduce air pollution by reducing car use, avoiding burning materials such as wood, and changing to energy efficient appliances.', 'I had to make a difficult decision when I was working as a project manager at a construction company. I was in charge of a project that needed to be completed by a certain date in order to meet the clientâ€™s expectations. However, due to unexpected delays, we were not able to meet the deadline and so I had to make a difficult decision. I decided to extend the deadline, but I had to stretch the teamâ€™s resources even further and increase the budget. Although it was a risky decision, I ultimately decided to go ahead with it to ensure that the project was completed on time and that the clientâ€™s expectations were met. The project was eventually successfully completed and this was seen as a testament to my leadership and decision-making abilities.'], 'text': ['Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGive three tips for staying healthy.\\n\\n### Response:\\n1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.', 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat are the three primary colors?\\n\\n### Response:\\nThe three primary colors are red, blue, and yellow.', 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nDescribe the structure of an atom.\\n\\n### Response:\\nAn atom is made up of a nucleus, which contains protons and neutrons, surrounded by electrons that travel in orbits around the nucleus. The protons and neutrons have a positive charge, while the electrons have a negative charge, resulting in an overall neutral atom. The number of each particle determines the atomic number and the type of atom.', 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nHow can we reduce air pollution?\\n\\n### Response:\\nThere are a number of ways to reduce air pollution, such as shifting to renewable energy sources, encouraging the use of public transportation, prohibiting the burning of fossil fuels, implementing policies to reduce emissions from industrial sources, and implementing vehicle emissions standards. Additionally, individuals can do their part to reduce air pollution by reducing car use, avoiding burning materials such as wood, and changing to energy efficient appliances.', 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nDescribe a time when you had to make a difficult decision.\\n\\n### Response:\\nI had to make a difficult decision when I was working as a project manager at a construction company. I was in charge of a project that needed to be completed by a certain date in order to meet the clientâ€™s expectations. However, due to unexpected delays, we were not able to meet the deadline and so I had to make a difficult decision. I decided to extend the deadline, but I had to stretch the teamâ€™s resources even further and increase the budget. Although it was a risky decision, I ultimately decided to go ahead with it to ensure that the project was completed on time and that the clientâ€™s expectations were met. The project was eventually successfully completed and this was seen as a testament to my leadership and decision-making abilities.']}\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"def format_alpaca_to_chatbot_text(example):\n    instruction = example['instruction']\n    input_text = example['input']\n    output_text = example['output']\n\n    # --- KHUÃ”N MáºªU Táº O CHUá»–I Äá»‚ FINETUNE LLM ---\n    # Sá»­ dá»¥ng Ä‘á»‹nh dáº¡ng tiÃªu chuáº©n (Standard Prompting Format)\n    \n    if input_text:\n        # TrÆ°á»ng há»£p cÃ³ Input/Ngá»¯ cáº£nh\n        text = (\n            f\"### User Instruction:\\n{instruction}\\n\\n\"\n            f\"### Context Input:\\n{input_text}\\n\\n\"\n            f\"### Assistant Response:\\n{output_text}\"\n        )\n    else:\n        # TrÆ°á»ng há»£p chá»‰ cÃ³ Instruction (thÆ°á»ng tháº¥y á»Ÿ Alpaca)\n        text = (\n            f\"### User Instruction:\\n{instruction}\\n\\n\"\n            f\"### Assistant Response:\\n{output_text}\"\n        )\n        \n    return {\"text\": text}\n\n# Sau Ä‘Ã³ Ã¡p dá»¥ng cho dataset:\n# alpaca_formatted = alpaca_dataset.map(format_alpaca_to_chatbot_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T14:50:48.831864Z","iopub.execute_input":"2025-10-18T14:50:48.832602Z","iopub.status.idle":"2025-10-18T14:50:48.837133Z","shell.execute_reply.started":"2025-10-18T14:50:48.832574Z","shell.execute_reply":"2025-10-18T14:50:48.836504Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Äá»‹nh nghÄ©a hÃ m tokenizer\nfrom transformers import LlamaTokenizer\n\ntokenizer = LlamaTokenizer(\n    vocab_file=\"/kaggle/input/final2-data/tinyshakespeare.model\",  # dÃ¹ng model luÃ´n\n    sp_model_file=\"/kaggle/input/final2-data/tinyshakespeare.model\",\n    unk_token=\"<unk>\",\n    bos_token=\"<s>\",\n    eos_token=\"</s>\",\n    pad_token=\"</s>\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T14:50:48.837877Z","iopub.execute_input":"2025-10-18T14:50:48.838067Z","iopub.status.idle":"2025-10-18T14:50:54.471516Z","shell.execute_reply.started":"2025-10-18T14:50:48.838051Z","shell.execute_reply":"2025-10-18T14:50:54.470969Z"}},"outputs":[{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"#Test\ntext = \"This is a test sentence.\"\ntokens = tokenizer.tokenize(text)\nprint(tokens)\nprint(tokenizer.convert_tokens_to_ids(tokens))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T14:50:54.472171Z","iopub.execute_input":"2025-10-18T14:50:54.472537Z","iopub.status.idle":"2025-10-18T14:50:54.477508Z","shell.execute_reply.started":"2025-10-18T14:50:54.472520Z","shell.execute_reply":"2025-10-18T14:50:54.476741Z"}},"outputs":[{"name":"stdout","text":"['â–This', 'â–is', 'â–a', 'â–test', 'â–sentence', '.']\n[470, 78, 5, 4224, 2381, 7961]\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Tokenize dá»¯ liá»‡u má»›i dá»±a trÃªn tokenize cÅ© \ndef tokenize(example):\n    return tokenizer(\n        example[\"text\"], \n        truncation=True, \n        padding=\"max_length\", \n        max_length=512\n    )\n\ntokenized_dataset = alpaca_dataset.map(tokenize, batched=True, remove_columns=[\"text\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T14:50:54.479074Z","iopub.execute_input":"2025-10-18T14:50:54.479282Z","iopub.status.idle":"2025-10-18T14:51:34.352026Z","shell.execute_reply.started":"2025-10-18T14:50:54.479267Z","shell.execute_reply":"2025-10-18T14:51:34.351180Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/52002 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33970cf66e454838936a22b77dcb935a"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"/kaggle/input/final2-data/\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T14:51:34.352692Z","iopub.execute_input":"2025-10-18T14:51:34.352961Z","iopub.status.idle":"2025-10-18T14:51:56.574535Z","shell.execute_reply.started":"2025-10-18T14:51:34.352926Z","shell.execute_reply":"2025-10-18T14:51:56.573688Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n2025-10-18 14:51:44.929474: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1760799105.118444      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1760799105.169351      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nSome weights of GPT2LMHeadModel were not initialized from the model checkpoint at /kaggle/input/final2-data/ and are newly initialized: ['lm_head.weight', 'transformer.h.0.attn.c_attn.bias', 'transformer.h.0.attn.c_attn.weight', 'transformer.h.0.attn.c_proj.bias', 'transformer.h.0.attn.c_proj.weight', 'transformer.h.0.ln_1.bias', 'transformer.h.0.ln_1.weight', 'transformer.h.0.ln_2.bias', 'transformer.h.0.ln_2.weight', 'transformer.h.0.mlp.c_fc.bias', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.0.mlp.c_proj.bias', 'transformer.h.0.mlp.c_proj.weight', 'transformer.h.1.attn.c_attn.bias', 'transformer.h.1.attn.c_attn.weight', 'transformer.h.1.attn.c_proj.bias', 'transformer.h.1.attn.c_proj.weight', 'transformer.h.1.ln_1.bias', 'transformer.h.1.ln_1.weight', 'transformer.h.1.ln_2.bias', 'transformer.h.1.ln_2.weight', 'transformer.h.1.mlp.c_fc.bias', 'transformer.h.1.mlp.c_fc.weight', 'transformer.h.1.mlp.c_proj.bias', 'transformer.h.1.mlp.c_proj.weight', 'transformer.h.10.attn.c_attn.bias', 'transformer.h.10.attn.c_attn.weight', 'transformer.h.10.attn.c_proj.bias', 'transformer.h.10.attn.c_proj.weight', 'transformer.h.10.ln_1.bias', 'transformer.h.10.ln_1.weight', 'transformer.h.10.ln_2.bias', 'transformer.h.10.ln_2.weight', 'transformer.h.10.mlp.c_fc.bias', 'transformer.h.10.mlp.c_fc.weight', 'transformer.h.10.mlp.c_proj.bias', 'transformer.h.10.mlp.c_proj.weight', 'transformer.h.11.attn.c_attn.bias', 'transformer.h.11.attn.c_attn.weight', 'transformer.h.11.attn.c_proj.bias', 'transformer.h.11.attn.c_proj.weight', 'transformer.h.11.ln_1.bias', 'transformer.h.11.ln_1.weight', 'transformer.h.11.ln_2.bias', 'transformer.h.11.ln_2.weight', 'transformer.h.11.mlp.c_fc.bias', 'transformer.h.11.mlp.c_fc.weight', 'transformer.h.11.mlp.c_proj.bias', 'transformer.h.11.mlp.c_proj.weight', 'transformer.h.2.attn.c_attn.bias', 'transformer.h.2.attn.c_attn.weight', 'transformer.h.2.attn.c_proj.bias', 'transformer.h.2.attn.c_proj.weight', 'transformer.h.2.ln_1.bias', 'transformer.h.2.ln_1.weight', 'transformer.h.2.ln_2.bias', 'transformer.h.2.ln_2.weight', 'transformer.h.2.mlp.c_fc.bias', 'transformer.h.2.mlp.c_fc.weight', 'transformer.h.2.mlp.c_proj.bias', 'transformer.h.2.mlp.c_proj.weight', 'transformer.h.3.attn.c_attn.bias', 'transformer.h.3.attn.c_attn.weight', 'transformer.h.3.attn.c_proj.bias', 'transformer.h.3.attn.c_proj.weight', 'transformer.h.3.ln_1.bias', 'transformer.h.3.ln_1.weight', 'transformer.h.3.ln_2.bias', 'transformer.h.3.ln_2.weight', 'transformer.h.3.mlp.c_fc.bias', 'transformer.h.3.mlp.c_fc.weight', 'transformer.h.3.mlp.c_proj.bias', 'transformer.h.3.mlp.c_proj.weight', 'transformer.h.4.attn.c_attn.bias', 'transformer.h.4.attn.c_attn.weight', 'transformer.h.4.attn.c_proj.bias', 'transformer.h.4.attn.c_proj.weight', 'transformer.h.4.ln_1.bias', 'transformer.h.4.ln_1.weight', 'transformer.h.4.ln_2.bias', 'transformer.h.4.ln_2.weight', 'transformer.h.4.mlp.c_fc.bias', 'transformer.h.4.mlp.c_fc.weight', 'transformer.h.4.mlp.c_proj.bias', 'transformer.h.4.mlp.c_proj.weight', 'transformer.h.5.attn.c_attn.bias', 'transformer.h.5.attn.c_attn.weight', 'transformer.h.5.attn.c_proj.bias', 'transformer.h.5.attn.c_proj.weight', 'transformer.h.5.ln_1.bias', 'transformer.h.5.ln_1.weight', 'transformer.h.5.ln_2.bias', 'transformer.h.5.ln_2.weight', 'transformer.h.5.mlp.c_fc.bias', 'transformer.h.5.mlp.c_fc.weight', 'transformer.h.5.mlp.c_proj.bias', 'transformer.h.5.mlp.c_proj.weight', 'transformer.h.6.attn.c_attn.bias', 'transformer.h.6.attn.c_attn.weight', 'transformer.h.6.attn.c_proj.bias', 'transformer.h.6.attn.c_proj.weight', 'transformer.h.6.ln_1.bias', 'transformer.h.6.ln_1.weight', 'transformer.h.6.ln_2.bias', 'transformer.h.6.ln_2.weight', 'transformer.h.6.mlp.c_fc.bias', 'transformer.h.6.mlp.c_fc.weight', 'transformer.h.6.mlp.c_proj.bias', 'transformer.h.6.mlp.c_proj.weight', 'transformer.h.7.attn.c_attn.bias', 'transformer.h.7.attn.c_attn.weight', 'transformer.h.7.attn.c_proj.bias', 'transformer.h.7.attn.c_proj.weight', 'transformer.h.7.ln_1.bias', 'transformer.h.7.ln_1.weight', 'transformer.h.7.ln_2.bias', 'transformer.h.7.ln_2.weight', 'transformer.h.7.mlp.c_fc.bias', 'transformer.h.7.mlp.c_fc.weight', 'transformer.h.7.mlp.c_proj.bias', 'transformer.h.7.mlp.c_proj.weight', 'transformer.h.8.attn.c_attn.bias', 'transformer.h.8.attn.c_attn.weight', 'transformer.h.8.attn.c_proj.bias', 'transformer.h.8.attn.c_proj.weight', 'transformer.h.8.ln_1.bias', 'transformer.h.8.ln_1.weight', 'transformer.h.8.ln_2.bias', 'transformer.h.8.ln_2.weight', 'transformer.h.8.mlp.c_fc.bias', 'transformer.h.8.mlp.c_fc.weight', 'transformer.h.8.mlp.c_proj.bias', 'transformer.h.8.mlp.c_proj.weight', 'transformer.h.9.attn.c_attn.bias', 'transformer.h.9.attn.c_attn.weight', 'transformer.h.9.attn.c_proj.bias', 'transformer.h.9.attn.c_proj.weight', 'transformer.h.9.ln_1.bias', 'transformer.h.9.ln_1.weight', 'transformer.h.9.ln_2.bias', 'transformer.h.9.ln_2.weight', 'transformer.h.9.mlp.c_fc.bias', 'transformer.h.9.mlp.c_fc.weight', 'transformer.h.9.mlp.c_proj.bias', 'transformer.h.9.mlp.c_proj.weight', 'transformer.ln_f.bias', 'transformer.ln_f.weight', 'transformer.wpe.weight', 'transformer.wte.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# compatible_training_verbose.py\nimport os\nimport math\nimport torch\nimport inspect\nfrom transformers import (\n    Trainer,\n    TrainingArguments,\n    DataCollatorForLanguageModeling,\n    TrainerCallback,\n)\n\n# --- 0ï¸âƒ£ Kiá»ƒm tra thiáº¿t bá»‹ ---\nif torch.cuda.is_available():\n    device_name = torch.cuda.get_device_name(0)\n    print(f\"ğŸš€ Using GPU: {device_name}\")\nelse:\n    print(\"âš ï¸ CUDA not available, using CPU\")\n\n# --- 1ï¸âƒ£ Chuáº©n bá»‹ dataset ---\ntrain_dataset = (\n    tokenized_dataset[\"train\"]\n    if isinstance(tokenized_dataset, dict) and \"train\" in tokenized_dataset\n    else tokenized_dataset\n)\n\n# --- 2ï¸âƒ£ Cáº¥u hÃ¬nh TrainingArguments ---\nargs_dict = dict(\n    output_dir=\"./alpaca_finetuned\",\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    learning_rate=2e-5,\n    num_train_epochs=3,\n    fp16=True,\n    save_strategy=\"epoch\",          # ğŸ’¾ lÆ°u má»—i epoch\n    save_total_limit=3,             # chá»‰ giá»¯ 3 checkpoint gáº§n nháº¥t\n    logging_steps=50,\n    report_to=\"none\",               # trÃ¡nh lá»—i khi khÃ´ng cÃ³ wandb\n    load_best_model_at_end=False,   # cÃ³ thá»ƒ báº­t náº¿u cÃ³ eval\n)\n\n# --- 3ï¸âƒ£ Táº¯t fp16 náº¿u khÃ´ng cÃ³ GPU ---\nif not torch.cuda.is_available():\n    args_dict.pop(\"fp16\", None)\n\n# --- 4ï¸âƒ£ Lá»c key há»£p lá»‡ ---\nsig = inspect.signature(TrainingArguments.__init__)\nsupported_keys = set(sig.parameters.keys()) - {\"self\"}\nfiltered_args = {k: v for k, v in args_dict.items() if k in supported_keys}\ntraining_args = TrainingArguments(**filtered_args)\n\n# --- 5ï¸âƒ£ Data collator ---\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\n# --- 6ï¸âƒ£ Callback in loss + perplexity ---\nclass LossLoggingCallback(TrainerCallback):\n    def on_train_begin(self, args, state, control, **kwargs):\n        print(\"\\nğŸ”¥ Training báº¯t Ä‘áº§u...\\n\")\n\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if logs and \"loss\" in logs:\n            loss = logs[\"loss\"]\n            ppl = math.exp(loss) if loss < 20 else float(\"inf\")\n            step = logs.get(\"step\", state.global_step)\n            epoch = state.epoch if state.epoch is not None else 0\n            print(f\"ğŸ§® [Epoch {epoch:.2f}] Step {step:>5}: loss = {loss:.4f} | ppl = {ppl:.2f}\")\n\n    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n        if metrics and \"eval_loss\" in metrics:\n            ppl = math.exp(metrics[\"eval_loss\"]) if metrics[\"eval_loss\"] < 20 else float(\"inf\")\n            print(f\"\\nğŸ“Š Evaluation - Loss: {metrics['eval_loss']:.4f} | Perplexity: {ppl:.2f}\\n\")\n\n# --- 7ï¸âƒ£ Táº¡o Trainer ---\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    callbacks=[LossLoggingCallback()],\n)\n\n# --- 8ï¸âƒ£ In loss trung bÃ¬nh ban Ä‘áº§u ---\ntry:\n    model.eval()\n    with torch.no_grad():\n        total_loss = 0\n        n_batches = 0\n        for i, batch in enumerate(train_dataset):\n            if i >= 10:\n                break\n            inputs = {k: torch.tensor(v).unsqueeze(0).to(model.device) for k, v in batch.items()}\n            outputs = model(**inputs)\n            loss = outputs.loss.item()\n            total_loss += loss\n            n_batches += 1\n        if n_batches > 0:\n            avg_loss = total_loss / n_batches\n            print(f\"\\nğŸ“ Average initial loss (sampled): {avg_loss:.4f} | PPL â‰ˆ {math.exp(avg_loss):.2f}\\n\")\nexcept Exception as e:\n    print(f\"âš ï¸ Could not compute initial loss preview: {e}\")\n\n# --- 9ï¸âƒ£ Kiá»ƒm tra checkpoint má»›i nháº¥t ---\nlatest_checkpoint = None\nif os.path.exists(training_args.output_dir):\n    checkpoints = [\n        os.path.join(training_args.output_dir, d)\n        for d in os.listdir(training_args.output_dir)\n        if d.startswith(\"checkpoint\")\n    ]\n    if checkpoints:\n        latest_checkpoint = max(checkpoints, key=os.path.getmtime)\n        print(f\"ğŸ“¦ Found checkpoint: {latest_checkpoint}\")\n\n# --- ğŸ”Ÿ Resume training ---\nif latest_checkpoint:\n    print(f\"ğŸ” Resuming from {latest_checkpoint} ...\")\n    trainer.train(resume_from_checkpoint=latest_checkpoint)\nelse:\n    print(\"ğŸ†• No checkpoint found, starting fresh training...\")\n    trainer.train()\n\nprint(\"\\nâœ… Training finished successfully!\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T14:51:56.575398Z","iopub.execute_input":"2025-10-18T14:51:56.576146Z","iopub.status.idle":"2025-10-18T17:43:06.231977Z","shell.execute_reply.started":"2025-10-18T14:51:56.576095Z","shell.execute_reply":"2025-10-18T17:43:06.231310Z"}},"outputs":[{"name":"stdout","text":"ğŸš€ Using GPU: Tesla P100-PCIE-16GB\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/2814855393.py:74: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"name":"stdout","text":"âš ï¸ Could not compute initial loss preview: new(): invalid data type 'str'\nğŸ†• No checkpoint found, starting fresh training...\n\nğŸ”¥ Training báº¯t Ä‘áº§u...\n\n","output_type":"stream"},{"name":"stderr","text":"`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='9753' max='9753' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9753/9753 2:51:05, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>5.736800</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>4.766200</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>4.316000</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>4.193800</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>4.032700</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>3.867800</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>3.856400</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>3.664400</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>3.530500</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>3.618000</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>3.610800</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>3.521800</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>3.446100</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>3.431800</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>3.379100</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>3.370300</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>3.386500</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>3.244200</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>3.224100</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>3.238300</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>3.210400</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>3.212900</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>3.228500</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>3.154100</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>3.152500</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>3.095500</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>3.121300</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>3.102200</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>3.084300</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>3.066900</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>3.055500</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>2.975400</td>\n    </tr>\n    <tr>\n      <td>1650</td>\n      <td>3.010200</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>2.991300</td>\n    </tr>\n    <tr>\n      <td>1750</td>\n      <td>3.047000</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>2.979000</td>\n    </tr>\n    <tr>\n      <td>1850</td>\n      <td>2.953000</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>2.963500</td>\n    </tr>\n    <tr>\n      <td>1950</td>\n      <td>2.950100</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>2.929900</td>\n    </tr>\n    <tr>\n      <td>2050</td>\n      <td>2.916500</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>2.915400</td>\n    </tr>\n    <tr>\n      <td>2150</td>\n      <td>2.926900</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>2.923200</td>\n    </tr>\n    <tr>\n      <td>2250</td>\n      <td>2.890700</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>2.869700</td>\n    </tr>\n    <tr>\n      <td>2350</td>\n      <td>2.900300</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>2.815300</td>\n    </tr>\n    <tr>\n      <td>2450</td>\n      <td>2.887200</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>2.883800</td>\n    </tr>\n    <tr>\n      <td>2550</td>\n      <td>2.945900</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>2.879800</td>\n    </tr>\n    <tr>\n      <td>2650</td>\n      <td>2.805100</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>2.826400</td>\n    </tr>\n    <tr>\n      <td>2750</td>\n      <td>2.832000</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>2.828200</td>\n    </tr>\n    <tr>\n      <td>2850</td>\n      <td>2.824200</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>2.714800</td>\n    </tr>\n    <tr>\n      <td>2950</td>\n      <td>2.768600</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>2.807500</td>\n    </tr>\n    <tr>\n      <td>3050</td>\n      <td>2.780000</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>2.728300</td>\n    </tr>\n    <tr>\n      <td>3150</td>\n      <td>2.796300</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>2.746800</td>\n    </tr>\n    <tr>\n      <td>3250</td>\n      <td>2.779200</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>2.718000</td>\n    </tr>\n    <tr>\n      <td>3350</td>\n      <td>2.753000</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>2.731000</td>\n    </tr>\n    <tr>\n      <td>3450</td>\n      <td>2.704500</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>2.739000</td>\n    </tr>\n    <tr>\n      <td>3550</td>\n      <td>2.689400</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>2.698100</td>\n    </tr>\n    <tr>\n      <td>3650</td>\n      <td>2.658600</td>\n    </tr>\n    <tr>\n      <td>3700</td>\n      <td>2.724200</td>\n    </tr>\n    <tr>\n      <td>3750</td>\n      <td>2.665500</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>2.658700</td>\n    </tr>\n    <tr>\n      <td>3850</td>\n      <td>2.689700</td>\n    </tr>\n    <tr>\n      <td>3900</td>\n      <td>2.682400</td>\n    </tr>\n    <tr>\n      <td>3950</td>\n      <td>2.692700</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>2.689700</td>\n    </tr>\n    <tr>\n      <td>4050</td>\n      <td>2.615100</td>\n    </tr>\n    <tr>\n      <td>4100</td>\n      <td>2.590800</td>\n    </tr>\n    <tr>\n      <td>4150</td>\n      <td>2.597400</td>\n    </tr>\n    <tr>\n      <td>4200</td>\n      <td>2.600800</td>\n    </tr>\n    <tr>\n      <td>4250</td>\n      <td>2.657900</td>\n    </tr>\n    <tr>\n      <td>4300</td>\n      <td>2.638000</td>\n    </tr>\n    <tr>\n      <td>4350</td>\n      <td>2.607800</td>\n    </tr>\n    <tr>\n      <td>4400</td>\n      <td>2.575000</td>\n    </tr>\n    <tr>\n      <td>4450</td>\n      <td>2.574200</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>2.599000</td>\n    </tr>\n    <tr>\n      <td>4550</td>\n      <td>2.580300</td>\n    </tr>\n    <tr>\n      <td>4600</td>\n      <td>2.583200</td>\n    </tr>\n    <tr>\n      <td>4650</td>\n      <td>2.576400</td>\n    </tr>\n    <tr>\n      <td>4700</td>\n      <td>2.599500</td>\n    </tr>\n    <tr>\n      <td>4750</td>\n      <td>2.551600</td>\n    </tr>\n    <tr>\n      <td>4800</td>\n      <td>2.561500</td>\n    </tr>\n    <tr>\n      <td>4850</td>\n      <td>2.534600</td>\n    </tr>\n    <tr>\n      <td>4900</td>\n      <td>2.565600</td>\n    </tr>\n    <tr>\n      <td>4950</td>\n      <td>2.617800</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>2.506200</td>\n    </tr>\n    <tr>\n      <td>5050</td>\n      <td>2.540300</td>\n    </tr>\n    <tr>\n      <td>5100</td>\n      <td>2.527100</td>\n    </tr>\n    <tr>\n      <td>5150</td>\n      <td>2.546100</td>\n    </tr>\n    <tr>\n      <td>5200</td>\n      <td>2.544200</td>\n    </tr>\n    <tr>\n      <td>5250</td>\n      <td>2.511300</td>\n    </tr>\n    <tr>\n      <td>5300</td>\n      <td>2.519700</td>\n    </tr>\n    <tr>\n      <td>5350</td>\n      <td>2.519100</td>\n    </tr>\n    <tr>\n      <td>5400</td>\n      <td>2.544400</td>\n    </tr>\n    <tr>\n      <td>5450</td>\n      <td>2.512400</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>2.466600</td>\n    </tr>\n    <tr>\n      <td>5550</td>\n      <td>2.516400</td>\n    </tr>\n    <tr>\n      <td>5600</td>\n      <td>2.490900</td>\n    </tr>\n    <tr>\n      <td>5650</td>\n      <td>2.504700</td>\n    </tr>\n    <tr>\n      <td>5700</td>\n      <td>2.531300</td>\n    </tr>\n    <tr>\n      <td>5750</td>\n      <td>2.524500</td>\n    </tr>\n    <tr>\n      <td>5800</td>\n      <td>2.502500</td>\n    </tr>\n    <tr>\n      <td>5850</td>\n      <td>2.526100</td>\n    </tr>\n    <tr>\n      <td>5900</td>\n      <td>2.490500</td>\n    </tr>\n    <tr>\n      <td>5950</td>\n      <td>2.472700</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>2.462200</td>\n    </tr>\n    <tr>\n      <td>6050</td>\n      <td>2.495800</td>\n    </tr>\n    <tr>\n      <td>6100</td>\n      <td>2.510400</td>\n    </tr>\n    <tr>\n      <td>6150</td>\n      <td>2.407300</td>\n    </tr>\n    <tr>\n      <td>6200</td>\n      <td>2.422200</td>\n    </tr>\n    <tr>\n      <td>6250</td>\n      <td>2.459600</td>\n    </tr>\n    <tr>\n      <td>6300</td>\n      <td>2.452000</td>\n    </tr>\n    <tr>\n      <td>6350</td>\n      <td>2.432200</td>\n    </tr>\n    <tr>\n      <td>6400</td>\n      <td>2.420800</td>\n    </tr>\n    <tr>\n      <td>6450</td>\n      <td>2.478000</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>2.466300</td>\n    </tr>\n    <tr>\n      <td>6550</td>\n      <td>2.410300</td>\n    </tr>\n    <tr>\n      <td>6600</td>\n      <td>2.447100</td>\n    </tr>\n    <tr>\n      <td>6650</td>\n      <td>2.471800</td>\n    </tr>\n    <tr>\n      <td>6700</td>\n      <td>2.410200</td>\n    </tr>\n    <tr>\n      <td>6750</td>\n      <td>2.376600</td>\n    </tr>\n    <tr>\n      <td>6800</td>\n      <td>2.406200</td>\n    </tr>\n    <tr>\n      <td>6850</td>\n      <td>2.414400</td>\n    </tr>\n    <tr>\n      <td>6900</td>\n      <td>2.315600</td>\n    </tr>\n    <tr>\n      <td>6950</td>\n      <td>2.405100</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>2.365200</td>\n    </tr>\n    <tr>\n      <td>7050</td>\n      <td>2.352200</td>\n    </tr>\n    <tr>\n      <td>7100</td>\n      <td>2.359600</td>\n    </tr>\n    <tr>\n      <td>7150</td>\n      <td>2.351400</td>\n    </tr>\n    <tr>\n      <td>7200</td>\n      <td>2.360400</td>\n    </tr>\n    <tr>\n      <td>7250</td>\n      <td>2.435900</td>\n    </tr>\n    <tr>\n      <td>7300</td>\n      <td>2.349500</td>\n    </tr>\n    <tr>\n      <td>7350</td>\n      <td>2.389500</td>\n    </tr>\n    <tr>\n      <td>7400</td>\n      <td>2.357200</td>\n    </tr>\n    <tr>\n      <td>7450</td>\n      <td>2.328300</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>2.410200</td>\n    </tr>\n    <tr>\n      <td>7550</td>\n      <td>2.394500</td>\n    </tr>\n    <tr>\n      <td>7600</td>\n      <td>2.366400</td>\n    </tr>\n    <tr>\n      <td>7650</td>\n      <td>2.408700</td>\n    </tr>\n    <tr>\n      <td>7700</td>\n      <td>2.405600</td>\n    </tr>\n    <tr>\n      <td>7750</td>\n      <td>2.402400</td>\n    </tr>\n    <tr>\n      <td>7800</td>\n      <td>2.354200</td>\n    </tr>\n    <tr>\n      <td>7850</td>\n      <td>2.410400</td>\n    </tr>\n    <tr>\n      <td>7900</td>\n      <td>2.405400</td>\n    </tr>\n    <tr>\n      <td>7950</td>\n      <td>2.385900</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>2.380900</td>\n    </tr>\n    <tr>\n      <td>8050</td>\n      <td>2.390100</td>\n    </tr>\n    <tr>\n      <td>8100</td>\n      <td>2.418300</td>\n    </tr>\n    <tr>\n      <td>8150</td>\n      <td>2.394900</td>\n    </tr>\n    <tr>\n      <td>8200</td>\n      <td>2.357500</td>\n    </tr>\n    <tr>\n      <td>8250</td>\n      <td>2.340000</td>\n    </tr>\n    <tr>\n      <td>8300</td>\n      <td>2.321300</td>\n    </tr>\n    <tr>\n      <td>8350</td>\n      <td>2.352500</td>\n    </tr>\n    <tr>\n      <td>8400</td>\n      <td>2.374600</td>\n    </tr>\n    <tr>\n      <td>8450</td>\n      <td>2.302700</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>2.351300</td>\n    </tr>\n    <tr>\n      <td>8550</td>\n      <td>2.339000</td>\n    </tr>\n    <tr>\n      <td>8600</td>\n      <td>2.327700</td>\n    </tr>\n    <tr>\n      <td>8650</td>\n      <td>2.372000</td>\n    </tr>\n    <tr>\n      <td>8700</td>\n      <td>2.344900</td>\n    </tr>\n    <tr>\n      <td>8750</td>\n      <td>2.402200</td>\n    </tr>\n    <tr>\n      <td>8800</td>\n      <td>2.355600</td>\n    </tr>\n    <tr>\n      <td>8850</td>\n      <td>2.338800</td>\n    </tr>\n    <tr>\n      <td>8900</td>\n      <td>2.352900</td>\n    </tr>\n    <tr>\n      <td>8950</td>\n      <td>2.330200</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>2.339700</td>\n    </tr>\n    <tr>\n      <td>9050</td>\n      <td>2.372200</td>\n    </tr>\n    <tr>\n      <td>9100</td>\n      <td>2.372300</td>\n    </tr>\n    <tr>\n      <td>9150</td>\n      <td>2.284400</td>\n    </tr>\n    <tr>\n      <td>9200</td>\n      <td>2.349100</td>\n    </tr>\n    <tr>\n      <td>9250</td>\n      <td>2.306600</td>\n    </tr>\n    <tr>\n      <td>9300</td>\n      <td>2.323200</td>\n    </tr>\n    <tr>\n      <td>9350</td>\n      <td>2.309000</td>\n    </tr>\n    <tr>\n      <td>9400</td>\n      <td>2.413800</td>\n    </tr>\n    <tr>\n      <td>9450</td>\n      <td>2.347400</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>2.333300</td>\n    </tr>\n    <tr>\n      <td>9550</td>\n      <td>2.370500</td>\n    </tr>\n    <tr>\n      <td>9600</td>\n      <td>2.330500</td>\n    </tr>\n    <tr>\n      <td>9650</td>\n      <td>2.336400</td>\n    </tr>\n    <tr>\n      <td>9700</td>\n      <td>2.363600</td>\n    </tr>\n    <tr>\n      <td>9750</td>\n      <td>2.371000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"ğŸ§® [Epoch 0.02] Step    50: loss = 5.7368 | ppl = 310.07\nğŸ§® [Epoch 0.03] Step   100: loss = 4.7662 | ppl = 117.47\nğŸ§® [Epoch 0.05] Step   150: loss = 4.3160 | ppl = 74.89\nğŸ§® [Epoch 0.06] Step   200: loss = 4.1938 | ppl = 66.27\nğŸ§® [Epoch 0.08] Step   250: loss = 4.0327 | ppl = 56.41\nğŸ§® [Epoch 0.09] Step   300: loss = 3.8678 | ppl = 47.84\nğŸ§® [Epoch 0.11] Step   350: loss = 3.8564 | ppl = 47.29\nğŸ§® [Epoch 0.12] Step   400: loss = 3.6644 | ppl = 39.03\nğŸ§® [Epoch 0.14] Step   450: loss = 3.5305 | ppl = 34.14\nğŸ§® [Epoch 0.15] Step   500: loss = 3.6180 | ppl = 37.26\nğŸ§® [Epoch 0.17] Step   550: loss = 3.6108 | ppl = 37.00\nğŸ§® [Epoch 0.18] Step   600: loss = 3.5218 | ppl = 33.85\nğŸ§® [Epoch 0.20] Step   650: loss = 3.4461 | ppl = 31.38\nğŸ§® [Epoch 0.22] Step   700: loss = 3.4318 | ppl = 30.93\nğŸ§® [Epoch 0.23] Step   750: loss = 3.3791 | ppl = 29.34\nğŸ§® [Epoch 0.25] Step   800: loss = 3.3703 | ppl = 29.09\nğŸ§® [Epoch 0.26] Step   850: loss = 3.3865 | ppl = 29.56\nğŸ§® [Epoch 0.28] Step   900: loss = 3.2442 | ppl = 25.64\nğŸ§® [Epoch 0.29] Step   950: loss = 3.2241 | ppl = 25.13\nğŸ§® [Epoch 0.31] Step  1000: loss = 3.2383 | ppl = 25.49\nğŸ§® [Epoch 0.32] Step  1050: loss = 3.2104 | ppl = 24.79\nğŸ§® [Epoch 0.34] Step  1100: loss = 3.2129 | ppl = 24.85\nğŸ§® [Epoch 0.35] Step  1150: loss = 3.2285 | ppl = 25.24\nğŸ§® [Epoch 0.37] Step  1200: loss = 3.1541 | ppl = 23.43\nğŸ§® [Epoch 0.38] Step  1250: loss = 3.1525 | ppl = 23.39\nğŸ§® [Epoch 0.40] Step  1300: loss = 3.0955 | ppl = 22.10\nğŸ§® [Epoch 0.42] Step  1350: loss = 3.1213 | ppl = 22.68\nğŸ§® [Epoch 0.43] Step  1400: loss = 3.1022 | ppl = 22.25\nğŸ§® [Epoch 0.45] Step  1450: loss = 3.0843 | ppl = 21.85\nğŸ§® [Epoch 0.46] Step  1500: loss = 3.0669 | ppl = 21.48\nğŸ§® [Epoch 0.48] Step  1550: loss = 3.0555 | ppl = 21.23\nğŸ§® [Epoch 0.49] Step  1600: loss = 2.9754 | ppl = 19.60\nğŸ§® [Epoch 0.51] Step  1650: loss = 3.0102 | ppl = 20.29\nğŸ§® [Epoch 0.52] Step  1700: loss = 2.9913 | ppl = 19.91\nğŸ§® [Epoch 0.54] Step  1750: loss = 3.0470 | ppl = 21.05\nğŸ§® [Epoch 0.55] Step  1800: loss = 2.9790 | ppl = 19.67\nğŸ§® [Epoch 0.57] Step  1850: loss = 2.9530 | ppl = 19.16\nğŸ§® [Epoch 0.58] Step  1900: loss = 2.9635 | ppl = 19.37\nğŸ§® [Epoch 0.60] Step  1950: loss = 2.9501 | ppl = 19.11\nğŸ§® [Epoch 0.62] Step  2000: loss = 2.9299 | ppl = 18.73\nğŸ§® [Epoch 0.63] Step  2050: loss = 2.9165 | ppl = 18.48\nğŸ§® [Epoch 0.65] Step  2100: loss = 2.9154 | ppl = 18.46\nğŸ§® [Epoch 0.66] Step  2150: loss = 2.9269 | ppl = 18.67\nğŸ§® [Epoch 0.68] Step  2200: loss = 2.9232 | ppl = 18.60\nğŸ§® [Epoch 0.69] Step  2250: loss = 2.8907 | ppl = 18.01\nğŸ§® [Epoch 0.71] Step  2300: loss = 2.8697 | ppl = 17.63\nğŸ§® [Epoch 0.72] Step  2350: loss = 2.9003 | ppl = 18.18\nğŸ§® [Epoch 0.74] Step  2400: loss = 2.8153 | ppl = 16.70\nğŸ§® [Epoch 0.75] Step  2450: loss = 2.8872 | ppl = 17.94\nğŸ§® [Epoch 0.77] Step  2500: loss = 2.8838 | ppl = 17.88\nğŸ§® [Epoch 0.78] Step  2550: loss = 2.9459 | ppl = 19.03\nğŸ§® [Epoch 0.80] Step  2600: loss = 2.8798 | ppl = 17.81\nğŸ§® [Epoch 0.82] Step  2650: loss = 2.8051 | ppl = 16.53\nğŸ§® [Epoch 0.83] Step  2700: loss = 2.8264 | ppl = 16.88\nğŸ§® [Epoch 0.85] Step  2750: loss = 2.8320 | ppl = 16.98\nğŸ§® [Epoch 0.86] Step  2800: loss = 2.8282 | ppl = 16.91\nğŸ§® [Epoch 0.88] Step  2850: loss = 2.8242 | ppl = 16.85\nğŸ§® [Epoch 0.89] Step  2900: loss = 2.7148 | ppl = 15.10\nğŸ§® [Epoch 0.91] Step  2950: loss = 2.7686 | ppl = 15.94\nğŸ§® [Epoch 0.92] Step  3000: loss = 2.8075 | ppl = 16.57\nğŸ§® [Epoch 0.94] Step  3050: loss = 2.7800 | ppl = 16.12\nğŸ§® [Epoch 0.95] Step  3100: loss = 2.7283 | ppl = 15.31\nğŸ§® [Epoch 0.97] Step  3150: loss = 2.7963 | ppl = 16.38\nğŸ§® [Epoch 0.98] Step  3200: loss = 2.7468 | ppl = 15.59\nğŸ§® [Epoch 1.00] Step  3250: loss = 2.7792 | ppl = 16.11\nğŸ§® [Epoch 1.02] Step  3300: loss = 2.7180 | ppl = 15.15\nğŸ§® [Epoch 1.03] Step  3350: loss = 2.7530 | ppl = 15.69\nğŸ§® [Epoch 1.05] Step  3400: loss = 2.7310 | ppl = 15.35\nğŸ§® [Epoch 1.06] Step  3450: loss = 2.7045 | ppl = 14.95\nğŸ§® [Epoch 1.08] Step  3500: loss = 2.7390 | ppl = 15.47\nğŸ§® [Epoch 1.09] Step  3550: loss = 2.6894 | ppl = 14.72\nğŸ§® [Epoch 1.11] Step  3600: loss = 2.6981 | ppl = 14.85\nğŸ§® [Epoch 1.12] Step  3650: loss = 2.6586 | ppl = 14.28\nğŸ§® [Epoch 1.14] Step  3700: loss = 2.7242 | ppl = 15.24\nğŸ§® [Epoch 1.15] Step  3750: loss = 2.6655 | ppl = 14.38\nğŸ§® [Epoch 1.17] Step  3800: loss = 2.6587 | ppl = 14.28\nğŸ§® [Epoch 1.18] Step  3850: loss = 2.6897 | ppl = 14.73\nğŸ§® [Epoch 1.20] Step  3900: loss = 2.6824 | ppl = 14.62\nğŸ§® [Epoch 1.22] Step  3950: loss = 2.6927 | ppl = 14.77\nğŸ§® [Epoch 1.23] Step  4000: loss = 2.6897 | ppl = 14.73\nğŸ§® [Epoch 1.25] Step  4050: loss = 2.6151 | ppl = 13.67\nğŸ§® [Epoch 1.26] Step  4100: loss = 2.5908 | ppl = 13.34\nğŸ§® [Epoch 1.28] Step  4150: loss = 2.5974 | ppl = 13.43\nğŸ§® [Epoch 1.29] Step  4200: loss = 2.6008 | ppl = 13.47\nğŸ§® [Epoch 1.31] Step  4250: loss = 2.6579 | ppl = 14.27\nğŸ§® [Epoch 1.32] Step  4300: loss = 2.6380 | ppl = 13.99\nğŸ§® [Epoch 1.34] Step  4350: loss = 2.6078 | ppl = 13.57\nğŸ§® [Epoch 1.35] Step  4400: loss = 2.5750 | ppl = 13.13\nğŸ§® [Epoch 1.37] Step  4450: loss = 2.5742 | ppl = 13.12\nğŸ§® [Epoch 1.38] Step  4500: loss = 2.5990 | ppl = 13.45\nğŸ§® [Epoch 1.40] Step  4550: loss = 2.5803 | ppl = 13.20\nğŸ§® [Epoch 1.42] Step  4600: loss = 2.5832 | ppl = 13.24\nğŸ§® [Epoch 1.43] Step  4650: loss = 2.5764 | ppl = 13.15\nğŸ§® [Epoch 1.45] Step  4700: loss = 2.5995 | ppl = 13.46\nğŸ§® [Epoch 1.46] Step  4750: loss = 2.5516 | ppl = 12.83\nğŸ§® [Epoch 1.48] Step  4800: loss = 2.5615 | ppl = 12.96\nğŸ§® [Epoch 1.49] Step  4850: loss = 2.5346 | ppl = 12.61\nğŸ§® [Epoch 1.51] Step  4900: loss = 2.5656 | ppl = 13.01\nğŸ§® [Epoch 1.52] Step  4950: loss = 2.6178 | ppl = 13.71\nğŸ§® [Epoch 1.54] Step  5000: loss = 2.5062 | ppl = 12.26\nğŸ§® [Epoch 1.55] Step  5050: loss = 2.5403 | ppl = 12.68\nğŸ§® [Epoch 1.57] Step  5100: loss = 2.5271 | ppl = 12.52\nğŸ§® [Epoch 1.58] Step  5150: loss = 2.5461 | ppl = 12.76\nğŸ§® [Epoch 1.60] Step  5200: loss = 2.5442 | ppl = 12.73\nğŸ§® [Epoch 1.62] Step  5250: loss = 2.5113 | ppl = 12.32\nğŸ§® [Epoch 1.63] Step  5300: loss = 2.5197 | ppl = 12.42\nğŸ§® [Epoch 1.65] Step  5350: loss = 2.5191 | ppl = 12.42\nğŸ§® [Epoch 1.66] Step  5400: loss = 2.5444 | ppl = 12.74\nğŸ§® [Epoch 1.68] Step  5450: loss = 2.5124 | ppl = 12.33\nğŸ§® [Epoch 1.69] Step  5500: loss = 2.4666 | ppl = 11.78\nğŸ§® [Epoch 1.71] Step  5550: loss = 2.5164 | ppl = 12.38\nğŸ§® [Epoch 1.72] Step  5600: loss = 2.4909 | ppl = 12.07\nğŸ§® [Epoch 1.74] Step  5650: loss = 2.5047 | ppl = 12.24\nğŸ§® [Epoch 1.75] Step  5700: loss = 2.5313 | ppl = 12.57\nğŸ§® [Epoch 1.77] Step  5750: loss = 2.5245 | ppl = 12.48\nğŸ§® [Epoch 1.78] Step  5800: loss = 2.5025 | ppl = 12.21\nğŸ§® [Epoch 1.80] Step  5850: loss = 2.5261 | ppl = 12.50\nğŸ§® [Epoch 1.82] Step  5900: loss = 2.4905 | ppl = 12.07\nğŸ§® [Epoch 1.83] Step  5950: loss = 2.4727 | ppl = 11.85\nğŸ§® [Epoch 1.85] Step  6000: loss = 2.4622 | ppl = 11.73\nğŸ§® [Epoch 1.86] Step  6050: loss = 2.4958 | ppl = 12.13\nğŸ§® [Epoch 1.88] Step  6100: loss = 2.5104 | ppl = 12.31\nğŸ§® [Epoch 1.89] Step  6150: loss = 2.4073 | ppl = 11.10\nğŸ§® [Epoch 1.91] Step  6200: loss = 2.4222 | ppl = 11.27\nğŸ§® [Epoch 1.92] Step  6250: loss = 2.4596 | ppl = 11.70\nğŸ§® [Epoch 1.94] Step  6300: loss = 2.4520 | ppl = 11.61\nğŸ§® [Epoch 1.95] Step  6350: loss = 2.4322 | ppl = 11.38\nğŸ§® [Epoch 1.97] Step  6400: loss = 2.4208 | ppl = 11.25\nğŸ§® [Epoch 1.98] Step  6450: loss = 2.4780 | ppl = 11.92\nğŸ§® [Epoch 2.00] Step  6500: loss = 2.4663 | ppl = 11.78\nğŸ§® [Epoch 2.01] Step  6550: loss = 2.4103 | ppl = 11.14\nğŸ§® [Epoch 2.03] Step  6600: loss = 2.4471 | ppl = 11.55\nğŸ§® [Epoch 2.05] Step  6650: loss = 2.4718 | ppl = 11.84\nğŸ§® [Epoch 2.06] Step  6700: loss = 2.4102 | ppl = 11.14\nğŸ§® [Epoch 2.08] Step  6750: loss = 2.3766 | ppl = 10.77\nğŸ§® [Epoch 2.09] Step  6800: loss = 2.4062 | ppl = 11.09\nğŸ§® [Epoch 2.11] Step  6850: loss = 2.4144 | ppl = 11.18\nğŸ§® [Epoch 2.12] Step  6900: loss = 2.3156 | ppl = 10.13\nğŸ§® [Epoch 2.14] Step  6950: loss = 2.4051 | ppl = 11.08\nğŸ§® [Epoch 2.15] Step  7000: loss = 2.3652 | ppl = 10.65\nğŸ§® [Epoch 2.17] Step  7050: loss = 2.3522 | ppl = 10.51\nğŸ§® [Epoch 2.18] Step  7100: loss = 2.3596 | ppl = 10.59\nğŸ§® [Epoch 2.20] Step  7150: loss = 2.3514 | ppl = 10.50\nğŸ§® [Epoch 2.21] Step  7200: loss = 2.3604 | ppl = 10.60\nğŸ§® [Epoch 2.23] Step  7250: loss = 2.4359 | ppl = 11.43\nğŸ§® [Epoch 2.25] Step  7300: loss = 2.3495 | ppl = 10.48\nğŸ§® [Epoch 2.26] Step  7350: loss = 2.3895 | ppl = 10.91\nğŸ§® [Epoch 2.28] Step  7400: loss = 2.3572 | ppl = 10.56\nğŸ§® [Epoch 2.29] Step  7450: loss = 2.3283 | ppl = 10.26\nğŸ§® [Epoch 2.31] Step  7500: loss = 2.4102 | ppl = 11.14\nğŸ§® [Epoch 2.32] Step  7550: loss = 2.3945 | ppl = 10.96\nğŸ§® [Epoch 2.34] Step  7600: loss = 2.3664 | ppl = 10.66\nğŸ§® [Epoch 2.35] Step  7650: loss = 2.4087 | ppl = 11.12\nğŸ§® [Epoch 2.37] Step  7700: loss = 2.4056 | ppl = 11.09\nğŸ§® [Epoch 2.38] Step  7750: loss = 2.4024 | ppl = 11.05\nğŸ§® [Epoch 2.40] Step  7800: loss = 2.3542 | ppl = 10.53\nğŸ§® [Epoch 2.41] Step  7850: loss = 2.4104 | ppl = 11.14\nğŸ§® [Epoch 2.43] Step  7900: loss = 2.4054 | ppl = 11.08\nğŸ§® [Epoch 2.45] Step  7950: loss = 2.3859 | ppl = 10.87\nğŸ§® [Epoch 2.46] Step  8000: loss = 2.3809 | ppl = 10.81\nğŸ§® [Epoch 2.48] Step  8050: loss = 2.3901 | ppl = 10.91\nğŸ§® [Epoch 2.49] Step  8100: loss = 2.4183 | ppl = 11.23\nğŸ§® [Epoch 2.51] Step  8150: loss = 2.3949 | ppl = 10.97\nğŸ§® [Epoch 2.52] Step  8200: loss = 2.3575 | ppl = 10.56\nğŸ§® [Epoch 2.54] Step  8250: loss = 2.3400 | ppl = 10.38\nğŸ§® [Epoch 2.55] Step  8300: loss = 2.3213 | ppl = 10.19\nğŸ§® [Epoch 2.57] Step  8350: loss = 2.3525 | ppl = 10.51\nğŸ§® [Epoch 2.58] Step  8400: loss = 2.3746 | ppl = 10.75\nğŸ§® [Epoch 2.60] Step  8450: loss = 2.3027 | ppl = 10.00\nğŸ§® [Epoch 2.61] Step  8500: loss = 2.3513 | ppl = 10.50\nğŸ§® [Epoch 2.63] Step  8550: loss = 2.3390 | ppl = 10.37\nğŸ§® [Epoch 2.65] Step  8600: loss = 2.3277 | ppl = 10.25\nğŸ§® [Epoch 2.66] Step  8650: loss = 2.3720 | ppl = 10.72\nğŸ§® [Epoch 2.68] Step  8700: loss = 2.3449 | ppl = 10.43\nğŸ§® [Epoch 2.69] Step  8750: loss = 2.4022 | ppl = 11.05\nğŸ§® [Epoch 2.71] Step  8800: loss = 2.3556 | ppl = 10.54\nğŸ§® [Epoch 2.72] Step  8850: loss = 2.3388 | ppl = 10.37\nğŸ§® [Epoch 2.74] Step  8900: loss = 2.3529 | ppl = 10.52\nğŸ§® [Epoch 2.75] Step  8950: loss = 2.3302 | ppl = 10.28\nğŸ§® [Epoch 2.77] Step  9000: loss = 2.3397 | ppl = 10.38\nğŸ§® [Epoch 2.78] Step  9050: loss = 2.3722 | ppl = 10.72\nğŸ§® [Epoch 2.80] Step  9100: loss = 2.3723 | ppl = 10.72\nğŸ§® [Epoch 2.81] Step  9150: loss = 2.2844 | ppl = 9.82\nğŸ§® [Epoch 2.83] Step  9200: loss = 2.3491 | ppl = 10.48\nğŸ§® [Epoch 2.85] Step  9250: loss = 2.3066 | ppl = 10.04\nğŸ§® [Epoch 2.86] Step  9300: loss = 2.3232 | ppl = 10.21\nğŸ§® [Epoch 2.88] Step  9350: loss = 2.3090 | ppl = 10.06\nğŸ§® [Epoch 2.89] Step  9400: loss = 2.4138 | ppl = 11.18\nğŸ§® [Epoch 2.91] Step  9450: loss = 2.3474 | ppl = 10.46\nğŸ§® [Epoch 2.92] Step  9500: loss = 2.3333 | ppl = 10.31\nğŸ§® [Epoch 2.94] Step  9550: loss = 2.3705 | ppl = 10.70\nğŸ§® [Epoch 2.95] Step  9600: loss = 2.3305 | ppl = 10.28\nğŸ§® [Epoch 2.97] Step  9650: loss = 2.3364 | ppl = 10.34\nğŸ§® [Epoch 2.98] Step  9700: loss = 2.3636 | ppl = 10.63\nğŸ§® [Epoch 3.00] Step  9750: loss = 2.3710 | ppl = 10.71\n\nâœ… Training finished successfully!\n\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n\n# âœ… Chá»‰ Ä‘á»‹nh Ä‘Ãºng checkpoint (trÃ¡nh lá»—i weight)\nmodel_path = \"./alpaca_finetuned/checkpoint-9753\"\n\n# âœ… Load tokenizer vÃ  model\ntokenizer = AutoTokenizer.from_pretrained(\"./alpaca_finetuned\")\nmodel = AutoModelForCausalLM.from_pretrained(model_path)\n\n# âœ… Resize phÃ²ng trÆ°á»ng há»£p token má»›i\nmodel.resize_token_embeddings(len(tokenizer))\n\n# âœ… Táº¡o pipeline cháº¡y CPU\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    device=\"cpu\"   # ğŸ‘ˆ Quan trá»ng\n)\n\n# âœ… Prompt test\nprompt = \"\"\"### Instruction:\nExplain what Artificial Intelligence is.\n\n### Input:\nNone\n\n### Response:\n\"\"\"\n\noutput = pipe(prompt, max_new_tokens=100, do_sample=True, temperature=0.7)[0][\"generated_text\"]\n\nprint(\"\\n=== OUTPUT ===\")\nprint(output)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T18:10:25.842249Z","iopub.execute_input":"2025-10-18T18:10:25.842710Z","iopub.status.idle":"2025-10-18T18:10:28.691677Z","shell.execute_reply.started":"2025-10-18T18:10:25.842687Z","shell.execute_reply":"2025-10-18T18:10:28.690903Z"}},"outputs":[{"name":"stderr","text":"Device set to use cpu\n","output_type":"stream"},{"name":"stdout","text":"\n=== OUTPUT ===\n### Instruction:\nExplain what Artificial Intelligence is.\n\n### Input:\nNone\n\n### Response:\nAI is its keyworks, and translation. AI uses in automate data and the user service that automate data, providing for targets to accuracy, and decisions. AI can help users with other handwords, allowing accuracy, and more accuracy, and to their data, and analyze and data and acc\n","output_type":"stream"}],"execution_count":33}]}