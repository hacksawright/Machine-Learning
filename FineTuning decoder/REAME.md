# ğŸ¦™ Fine-tuning Decoder on Alpaca Dataset

## ğŸ“˜ Tá»•ng quan

Dá»± Ã¡n nÃ y thá»±c hiá»‡n **fine-tuning láº¡i má»™t mÃ´ hÃ¬nh ngÃ´n ngá»¯ (decoder)** Ä‘Ã£ Ä‘Æ°á»£c **huáº¥n luyá»‡n ban Ä‘áº§u trÃªn táº­p dá»¯ liá»‡u Tiny Shakespeare**, nháº±m giÃºp mÃ´ hÃ¬nh há»c Ä‘Æ°á»£c **kháº£ nÄƒng pháº£n há»“i theo kiá»ƒu há»™i thoáº¡i (instruction-following)** tá»« **táº­p dá»¯ liá»‡u Alpaca**.

Káº¿t quáº£ mong Ä‘á»£i lÃ  mÃ´ hÃ¬nh cÃ³ thá»ƒ tráº£ lá»i cÃ¢u há»i, lÃ m theo yÃªu cáº§u, vÃ  pháº£n há»“i tá»± nhiÃªn nhÆ° má»™t chatbot huáº¥n luyá»‡n theo hÆ°á»›ng dáº«n.

---

## âš™ï¸ Cáº¥u trÃºc Notebook

Quy trÃ¬nh trong file `finetuning-decoder.ipynb` bao gá»“m cÃ¡c bÆ°á»›c chÃ­nh:

### 1ï¸âƒ£ Chuáº©n bá»‹ dá»¯ liá»‡u Alpaca

```python
from datasets import load_dataset

alpaca_dataset = load_dataset("tatsu-lab/alpaca")
print(alpaca_dataset)

```

- Táº­p dá»¯ liá»‡u gá»“m ~52,000 máº«u.
- Má»—i máº«u chá»©a:
    - `instruction`: yÃªu cáº§u tá»« ngÆ°á»i dÃ¹ng
    - `input`: ngá»¯ cáº£nh (cÃ³ thá»ƒ rá»—ng)
    - `output`: pháº£n há»“i tÆ°Æ¡ng á»©ng
- Dá»¯ liá»‡u Ä‘Æ°á»£c Ä‘á»‹nh dáº¡ng láº¡i theo cáº¥u trÃºc chuáº©n phá»¥c vá»¥ huáº¥n luyá»‡n chatbot.

VÃ­ dá»¥:

```
### User Instruction:
Give three tips for staying healthy.

### Assistant Response:
1. Eat a balanced diet...
2. Exercise regularly...
3. Get enough sleep...

```

---

### 2ï¸âƒ£ Tiá»n xá»­ lÃ½ vÃ  Ä‘á»‹nh dáº¡ng

HÃ m Ä‘á»‹nh dáº¡ng dá»¯ liá»‡u:

```python
def format_alpaca_to_chatbot_text(example):
    instruction = example['instruction']
    input_text = example['input']
    output_text = example['output']

    # --- KHUÃ”N MáºªU Táº O CHUá»–I Äá»‚ FINETUNE LLM ---
    # Sá»­ dá»¥ng Ä‘á»‹nh dáº¡ng tiÃªu chuáº©n (Standard Prompting Format)
    
    if input_text:
        # TrÆ°á»ng há»£p cÃ³ Input/Ngá»¯ cáº£nh
        text = (
            f"### User Instruction:\n{instruction}\n\n"
            f"### Context Input:\n{input_text}\n\n"
            f"### Assistant Response:\n{output_text}"
        )
    else:
        # TrÆ°á»ng há»£p chá»‰ cÃ³ Instruction (thÆ°á»ng tháº¥y á»Ÿ Alpaca)
        text = (
            f"### User Instruction:\n{instruction}\n\n"
            f"### Assistant Response:\n{output_text}"
        )
        
    return {"text": text}
```

- Chuáº©n hÃ³a má»—i máº«u thÃ nh Ä‘oáº¡n vÄƒn báº£n hoÃ n chá»‰nh theo format chuáº©n cá»§a LLM instruction-tuning.
- Káº¿t quáº£ Ä‘Æ°á»£c lÆ°u láº¡i trong trÆ°á»ng `"text"` Ä‘á»ƒ dÃ¹ng cho tokenizer.

---

### 3ï¸âƒ£ Sá»­ dá»¥ng tokenizer cá»§a mÃ´ hÃ¬nh gá»‘c (Tiny Shakespeare)

- Tokenizer gá»‘c Ä‘Æ°á»£c giá»¯ nguyÃªn Ä‘á»ƒ Ä‘áº£m báº£o tÃ­nh tÆ°Æ¡ng thÃ­ch giá»¯a mÃ´ hÃ¬nh cÅ© vÃ  má»›i.
    
    ```python
    from transformers import LlamaTokenizer
    
    tokenizer = LlamaTokenizer(
        vocab_file="/kaggle/input/final2-data/tinyshakespeare.model",
        sp_model_file="/kaggle/input/final2-data/tinyshakespeare.model",
        unk_token="<unk>", bos_token="<s>", eos_token="</s>", pad_token="</s>"
    )
    
    ```
    
- Äiá»u nÃ y giÃºp fine-tuning chá»‰ táº­p trung Ä‘iá»u chá»‰nh **trá»ng sá»‘ cá»§a mÃ´ hÃ¬nh**, khÃ´ng lÃ m thay Ä‘á»•i tá»« vá»±ng ban Ä‘áº§u.

---

### 4ï¸âƒ£ Fine-tuning mÃ´ hÃ¬nh

- Dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c token hÃ³a vÃ  Ä‘Ã³ng gÃ³i vÃ o `DataLoader`.
- MÃ´ hÃ¬nh decoder (Ä‘Ã£ pretrain trÃªn Tiny Shakespeare) Ä‘Æ°á»£c load lÃªn CPU hoáº·c GPU (tuá»³ mÃ´i trÆ°á»ng).
- Thá»±c hiá»‡n fine-tuning vá»›i hyperparameter cÆ¡ báº£n:
    - Learning rate nhá» (vÃ¬ Ä‘Ã¢y lÃ  fine-tune, khÃ´ng pháº£i train tá»« Ä‘áº§u).
    - Batch size nhá» vá»«a Ä‘á»ƒ trÃ¡nh quÃ¡ táº£i GPU.
    - Epochs tá»« 3.

---

### 5ï¸âƒ£ ÄÃ¡nh giÃ¡ & sinh vÄƒn báº£n thá»­ nghiá»‡m

Sau khi huáº¥n luyá»‡n xong, mÃ´ hÃ¬nh Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ sinh thá»­ pháº£n há»“i vá»›i má»™t prompt má»›i:

```
### Instruction:
Explain what Artificial Intelligence is.

### Input:
None

### Response:
AI is its keyworks, and translation. AI uses in automate data and the user service that automate data, providing for targets to accuracy, and decisions. AI can help users with other handwords, allowing accuracy, and more accuracy, and to their data, and analyze and data and ac
```

Káº¿t quáº£ cho tháº¥y mÃ´ hÃ¬nh Ä‘Ã£ há»c cÃ¡ch sinh vÄƒn báº£n tá»± nhiÃªn hÆ¡n, khÃ´ng bá»‹ láº·p, vÃ  cÃ³ kháº£ nÄƒng tráº£ lá»i theo dáº¡ng â€œAssistant Responseâ€.

---

## ğŸ§  Ã nghÄ©a cá»§a dá»± Ã¡n

- **Model gá»‘c:** há»c ngá»¯ phÃ¡p, cáº¥u trÃºc cÃ¢u, vÃ  phong cÃ¡ch ngÃ´n ngá»¯ tá»± nhiÃªn tá»« *Tiny Shakespeare*.
- **Fine-tuning vá»›i Alpaca:** giÃºp mÃ´ hÃ¬nh chuyá»ƒn tá»« viáº¿t vÄƒn báº£n theo phong cÃ¡ch thÆ¡ ká»‹ch cá»§a táº­p dá»¯ liá»‡u tinyshakespeare sang â€œtráº£ lá»i theo hÆ°á»›ng dáº«nâ€ (instruction-following).
- **Káº¿t quáº£:** mÃ´ hÃ¬nh sau fine-tuning vá»«a giá»¯ kháº£ nÄƒng viáº¿t máº¡ch láº¡c, vá»«a cÃ³ thá»ƒ pháº£n há»“i nhÆ° chatbot thÃ´ng minh.